# PPO（Proximal Policy Optimization）（TensorFlow2.3.0を使用）


## 概要

PPO（Proximal Policy Optimization）をTensorflow2.3.0で実装しました。<br>
**PPOでは普通に行われる並列化Agentによる経験データ（Trajectory）収集は行っていません。Agentは単体です。**<BR>
    
本モデルは、行動が連続値を取る環境を対象としています。<BR>
本稿では、OpenAI GymのBipedalWalkerを使用しています。<br>

**※理論の説明はしていません。他のリソースを参考にしてください。**<br>
**&nbsp;&nbsp; ネットや書籍でなかなか明示されておらず、私自身が実装に際し収集や理解に不便を感じたものを記載しています。**

<br>
    
### 使用した環境
本稿の実験においては、以下の環境を使用しています。<br>

| 環境名 | 外観| 状態の次元 | 行動の次元 |1エピソードでの上限ステップ数|目的|
|      :---:     |      :---:      |     :---:     |     :---:      |     :---:     |     :---:     |
|BipedalWalker|![BipedalWalker_mini](https://user-images.githubusercontent.com/52105933/95576368-4cba7a80-0a6b-11eb-922e-52c584a8915e.png)|24|4|2000|2足歩行して遠くまで行く|

<br>
    
## 実装の要点

### 全体の構成

Trainerは、AgentをPPOメソッドに従い訓練します。<br>
Agentの中身は、Actor-Criticです（両者独立したNN）。<BR>
PPOでは普通に行われる並列化Agentによる経験データ収集は行っていません。Agentは単体です。

![全体構成図_80](https://user-images.githubusercontent.com/52105933/110124918-0db80780-7e06-11eb-9b60-61979ea59870.png)

#### Agentの構成

Actor-Criticで、ActorとCriticは別々のNNにしています。<BR>

**Actor側では、行動はガウス方策に従います。<br>
よって、Actorの出力は、正規分布の平均μと標準偏差σとなります。**<br>
行動の次元数をK(=4)とすると、平均μと標準偏差σの1セットがK(=4)個ある、ということになります。<br>

Critic側は、単純に状態価値関数値を出力するだけです。<br>

**Actor側、Critic側双方で、GAE（Generalized Advantage Estimation）を使用**しています。

![NN構成図_70](https://user-images.githubusercontent.com/52105933/100530464-c71aa600-3235-11eb-9c98-2174015889b9.png)

<br>

### 訓練処理

#### 流れ

1エポック内で、以下のことを上から順に行います。<br>
- （初回エポックのみ）経験データ（Trajectory）収集<br>
- Agentの訓練<br>
- 経験データ（Trajectory）収集<br>
このエポックの評価と、次のエポックでのAgentの訓練のため

これを複数エポック繰り返します。

![訓練処理の時間軸_70](https://user-images.githubusercontent.com/52105933/110056276-dadc2800-7da1-11eb-99da-50b54fb14aef.png)

#### 報酬設計の変更　～ gym.Wrapperのサブクラス

BipedalWalkerは、失敗時（転倒時）、-100という即時報酬を返してエピソードを終了します。<br>
このように一連の経験データ中に数値規模が著しく大きい即時報酬がポツンポツンとある場合、それは実質”外れ値”となります。<br>
後述する「即時報酬の標準化」を通すと、他の標準化報酬の数値規模が著しく小さくなってしまいます。<br>
よって、gym.Wrapperのサブクラスを作成し、以下のような即時報酬を返すようにしました。
|事象|即時報酬|
|      :---:     |      :---:      | 
|エピソード途中のステップ|オリジナルのgym.Wrapperのrewardと同じ|
|エピソード終端　成功時<br>（2000ステップ転倒しなかった）|+1|
|エピソード終端　失敗時<br>（2000ステップもたず転倒した）|-1|

#### ratio（「rt(Θ)」）

ActorのLossをclipするのに使用されるratioは、以下のように算出しています。

![ratio説明_80](https://user-images.githubusercontent.com/52105933/110090892-29f27f00-7ddb-11eb-9abb-301985835e45.png)

#### 即時報酬の標準化

稼得した即時報酬の標準化を行っています。<BR>
この標準化された即時報酬を使用して、GAE（Generalized Advantage Estimation）の算出をします。<BR>
ただし、この標準化において、平均を引き算しません。<BR>
平均を引くと、実際の即時報酬とはプラスマイナスの符号が逆になってしまう標準化報酬が出てきてしまうからです。<BR>
また、標準化する対象範囲は、1エポックで収集する経験データのみの即時報酬ではなく（エポック毎に経験データを洗い替える）、訓練開始からの全経験データの即時報酬としています。<BR>
同一環境内での、訓練課程での報酬の数値規模の違いの吸収、を考慮してこのようにしました。

<br><br>

## 訓練結果

### 方策エントロピー補正無し　の場合

まずは、方策エントロピー補正無しで、訓練してみました。

#### 訓練の設定値など

|項目|値など|
|      :---:     |      :---:      | 
|経験データサイズ|20480|
|バッチサイズ|2048|
|イテレーション回数/エポック|10|
|Clip Rangeのε|0.2|
|GAE算出でのλ|0.95|
|GAE算出での報酬の割引率γ|0.99|
|方策エントロピー補正項の係数c|**0**|

#### 各エポックでの訓練成果の評価グラフ

![epoch推移グラフ群_80](https://user-images.githubusercontent.com/52105933/110078652-674f1080-7dcb-11eb-8512-a68a0083aba4.png)

#### 考察

以下のような結果になりました。<br>
- 1エポックあたりの稼得報酬は、途中まで増加した後、不安定になっている<br>
- 稼得報酬の1エピソード平均は、ゆるやかに増加している<br>
- ステップ数の1エピソード平均は、あるタイミングで少し増加した後、不安定になっている

また、グラフからは分かりにくいですが、ある程度訓練が進むと、「ステップ数エピソード平均が大きいと、そのエポックの稼得報酬合計が少な目」という傾向になっています（グラフを跨いだ縦の点線を参考に）。<br>
これらのことから、1ステップあたりの報酬はマイナスになることが多く、「傷口が広がる前に早めにコケて稼得報酬合計の高さを維持する」ことを学んでしまったのか、と推測しました。<br>
この状況を打破するには、「1つの方策に凝り固まることなく、いろんな手を打たせる」のが有効か、と推測しました。<br>
こうして、下記の方策エントロピー補正を行うことにしました。<br>
<br>


### 方策エントロピー補正有り　の場合

次に、方策エントロピー補正有りで、訓練してみました。

#### 方策エントロピー補正項

Actor（Policy側）のLossにおいて、以下のように方策エントロピー項をマイナスします。<BR>
マイナスするのは、方策が確定的でない場合に比べて、方策が確定的である場合のLossが大きくなるようにするためです。<br>

![方策エントロピー補正_80](https://user-images.githubusercontent.com/52105933/110091449-ccaafd80-7ddb-11eb-8f2f-991cdbe65ac2.png)

#### 訓練の設定値など

方策エントロピー補正項の係数c以外、全て同じです。

|項目|値など|
|      :---:     |      :---:      | 
|経験データサイズ|20480|
|バッチサイズ|2048|
|イテレーション回数/エポック|10|
|Clip Rangeのε|0.2|
|GAE算出でのλ|0.95|
|GAE算出での報酬の割引率γ|0.99|
|方策エントロピー補正項の係数c|**0.1**|

#### 各エポックでの訓練成果の評価グラフ

![epoch推移グラフ群_80_W](https://user-images.githubusercontent.com/52105933/110111228-574b2700-7df3-11eb-8497-d517170b278f.png)

#### 考察

以下のような結果になりました。<br>
- 1エポックあたりの稼得報酬については、方策エントロピー補正有りの方は、常に無しより多く、且つ増加傾向が止まることが無い。100エポック付近から増加スピードが遅くなる。<br>
- 稼得報酬の1エピソード平均については、方策エントロピー補正有りの方は、ほぼ常に無しより多く、増加スピードも速い。<br>
- ステップ数の1エピソード平均については、方策エントロピー補正有りの方は、無しに比べて高かったり低かったりしているが、最終的には高くなっている。<BR>
さらに、大局的に見て、100エポック付近から増加傾向を常に維持している。<BR>
- Actor（Policy側）のLossについては、方策エントロピー補正の有無では変わりがない。<br>
- Critic（Value側）のLossについては、方策エントロピー有りの方は、無しに比べて、100エポック付近からは常に高く、且つ大局的に見て増加傾向がある。

「1エポックあたりの稼得報酬」「稼得報酬の1エピソード平均」「ステップ数の1エピソード平均」の3つすべてがほぼ最高値という3冠王的エポック（グラフの緑の〇）が出てきており、「報酬とステップ数をともに大きく」というBipedalWalkerのタスクの目的に、訓練の方向が正しく向いているように見えます。<br>
つまり、「ステップ数エピソード平均が大きいと、そのエポックの稼得報酬合計が少な目」という傾向は、概ね解消されているように見えます（ただそのようになっているエポックもまだ残っています）。<br>

Critic（Value側）のLossがむしろ増加傾向になった、というのは、稼得報酬が増えたからでは、と思います。<br>
Criticでは、GAE + 出力値そのもの　が教師信号です。これは、GAEの2乗がそのままCriticのLossになる、ということです。つまり、GAEが大きくなるほどCriticのLossも大きくなります。<BR>

ただ、この結果をもって、**「方策エントロピー補正の効果があった」とは断言できない**です。<br>
方策エントロピー補正は、Actor（Policy側）のLoss数値を変えるのですが、そのLossに方策エントロピー補正の有無での変化がない、というのは奇異に感じます（だからと言って、補正の効果が無かったとも断定できないと思いますが）。<br>
実は、方策エントロピー補正無しの訓練を2回、方策エントロピー補正有りの訓練を2回行いました。各々、2回のうち1回は訓練がほとんど進まず、途中で中止しました。本稿に掲載しているデータは、うまくいった1回の方です。<br>
よって、本稿に掲載している結果は「たまたまそうなった」ものである可能性があります。<br>
本来ならば、同じハイパーパラメーターで何回か訓練し、結果を平均する、ということをすべきと思います。しかし、私のような個人が、1回8～9時間もかかる訓練を何度も行うのは不可能です。<br>
よって、現時点では、**「方策エントロピー補正は効果がある可能性がある、ということを確認した」としか言えない**です。<br>

ちなみに、「訓練1回に8～9時間もかかるのは、並列化Agentによる経験データ（Trajectory）収集を行っていないからでは」という批評は、きっと妥当なのだと思います。<br>
ですが、経験データ収集にAgent並列化を用いてどれほどの速度改善効果が得られるのか、イマイチ確信が持てませんでした。<br>
言うまでもなく、Agent並列化は、CPU/GPUを増やすわけではなく、それらを複数のAgentがうまく”共同使用”するものです。Agentを2体にして時間が半分になるわけではないです。<br>
また、仮にAgent並列化で経験データ収集が早く済むとしても、訓練はそもそも経験データ収集だけではなく、順伝播→逆伝播→パラメーターー更新、という重いプロセスがその後にあります。つまり、効果は限定的である可能性があります。<br>
Google Colaboratory固有の問題もあります。私のような個人は、無料のGoogle Colaboratoryを多用するしかありません。そして、Agent並列化は、メモリを大量に消費するはずです。
周知の通り、Google Colaboratoryは無料であるが故、「メモリが足りない」「一定時間操作が無い」などとしょっちゅうワガママを言っては勝手に稼働を打ち切ってしまいます。
こんな「安かろう悪かろう」な環境で、並列化したAgentなんてマトモに動くのか、疑問に思いました。<br>
一方で、Agent並列化は、（私にとって）難易度が高い＝costが高いものです。<br>
つまり、「costは高く、benefitは不明」なものに、その高いcostを投入することは、一個人に過ぎない私にはできなかったです。

<br><br>

## 物理構成

※TensorFlowは2.3.0以上が必要です。tensorflow_probabilityを使用しています。

training.ipynb　・・・実際にモデルの訓練を実行できるノートブック<BR>
PPOAgentTrainer.py　・・・Trainer<BR>
Agent.py　・・・Agent<BR>
common/<br>
&nbsp;└funcs.py　・・・ユーティリティ関数など<br>
&nbsp;└env_wrappers.py　・・・gym.Wrapperの各種サブクラス<br>
<br>

![物理構成_70](https://user-images.githubusercontent.com/52105933/100743622-e2490980-341f-11eb-98ce-c7c10d18d438.png)

<br><br><br>


※本リポジトリに公開しているプログラムやデータ、リンク先の情報の利用によって生じたいかなる損害の責任も負いません。これらの利用は、利用者の責任において行ってください。